{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/longnguyenQB/Heart-disease-predict/blob/main/fill_VN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqeKPNKGiFgx"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe3TBFOQj2wU",
        "outputId": "60d08c99-3457-4d14-f555-51bd4aee1296"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Io6UmXxORMp"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/attardi/wikiextractor.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install wikiextractor"
      ],
      "metadata": {
        "id": "qG2jHot0l4ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3U98AdBCQOF6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7728cf5c-14c6-4426-f76b-0c596574fc19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Preprocessing 'viwiki-20220401-pages-articles-multistream.xml.bz2' to collect template definitions: this may take some time.\n",
            "INFO: Preprocessed 100000 pages\n",
            "INFO: Preprocessed 200000 pages\n",
            "INFO: Preprocessed 300000 pages\n",
            "INFO: Preprocessed 400000 pages\n",
            "INFO: Preprocessed 500000 pages\n",
            "INFO: Preprocessed 600000 pages\n",
            "INFO: Preprocessed 700000 pages\n",
            "INFO: Preprocessed 800000 pages\n",
            "INFO: Preprocessed 900000 pages\n",
            "INFO: Preprocessed 1000000 pages\n",
            "INFO: Preprocessed 1100000 pages\n",
            "INFO: Preprocessed 1200000 pages\n",
            "INFO: Preprocessed 1300000 pages\n",
            "INFO: Preprocessed 1400000 pages\n",
            "INFO: Preprocessed 1500000 pages\n",
            "INFO: Preprocessed 1600000 pages\n",
            "INFO: Preprocessed 1700000 pages\n",
            "INFO: Preprocessed 1800000 pages\n",
            "INFO: Preprocessed 1900000 pages\n",
            "INFO: Preprocessed 2000000 pages\n",
            "INFO: Preprocessed 2100000 pages\n",
            "INFO: Loaded 206666 templates in 334.2s\n",
            "INFO: Starting page extraction from viwiki-20220401-pages-articles-multistream.xml.bz2.\n",
            "INFO: Using 4 extract processes.\n",
            "> /content/drive/MyDrive/Colab Notebooks/wikiextractor/wikiextractor/extract.py(86)clean()\n",
            "-> text = extractor.expandTemplates(text)\n",
            "> /content/drive/MyDrive/Colab Notebooks/wikiextractor/wikiextractor/extract.py(86)clean()\n",
            "-> text = extractor.expandTemplates(text)\n",
            "(Pdb) \n",
            "Process ForkProcess-4:\n",
            "(Pdb) > /content/drive/MyDrive/Colab Notebooks/wikiextractor/wikiextractor/extract.py(86)clean()\n",
            "-> text = extractor.expandTemplates(text)\n",
            "\n",
            "(Pdb) > /content/drive/MyDrive/Colab Notebooks/wikiextractor/wikiextractor/extract.py(86)clean()\n",
            "-> text = extractor.expandTemplates(text)\n",
            "(Pdb) Process ForkProcess-2:\n",
            "\n",
            "\n",
            "Process ForkProcess-5:\n",
            "Process ForkProcess-3:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"wikiextractor/WikiExtractor.py\", line 473, in extract_process\n",
            "    Extractor(*job[:-1]).extract(out, html_safe)  # (id, urlbase, title, page)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/wikiextractor/wikiextractor/extract.py\", line 976, in extract\n",
            "    text = self.clean_text(text, html_safe=html_safe)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/wikiextractor/wikiextractor/extract.py\", line 964, in clean_text\n",
            "    html_safe=html_safe)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/wikiextractor/wikiextractor/extract.py\", line 86, in clean\n",
            "    text = extractor.expandTemplates(text)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/wikiextractor/wikiextractor/extract.py\", line 86, in clean\n",
            "    text = extractor.expandTemplates(text)\n",
            "  File \"/usr/lib/python3.7/bdb.py\", line 88, in trace_dispatch\n",
            "    return self.dispatch_line(frame)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/bdb.py\", line 113, in dispatch_line\n",
            "    if self.quitting: raise BdbQuit\n",
            "bdb.BdbQuit\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"wikiextractor/WikiExtractor.py\", line 473, in extract_process\n",
            "    Extractor(*job[:-1]).extract(out, html_safe)  # (id, urlbase, title, page)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/wikiextractor/wikiextractor/extract.py\", line 976, in extract\n",
            "    text = self.clean_text(text, html_safe=html_safe)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/wikiextractor/wikiextractor/extract.py\", line 964, in clean_text\n",
            "    html_safe=html_safe)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/wikiextractor/wikiextractor/extract.py\", line 86, in clean\n",
            "    text = extractor.expandTemplates(text)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/wikiextractor/wikiextractor/extract.py\", line 86, in clean\n",
            "    text = extractor.expandTemplates(text)\n",
            "  File \"/usr/lib/python3.7/bdb.py\", line 88, in trace_dispatch\n",
            "    return self.dispatch_line(frame)\n",
            "  File \"/usr/lib/python3.7/bdb.py\", line 113, in dispatch_line\n",
            "    if self.quitting: raise BdbQuit\n",
            "bdb.BdbQuit\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"wikiextractor/WikiExtractor.py\", line 473, in extract_process\n",
            "    Extractor(*job[:-1]).extract(out, html_safe)  # (id, urlbase, title, page)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/wikiextractor/wikiextractor/extract.py\", line 976, in extract\n",
            "    text = self.clean_text(text, html_safe=html_safe)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/wikiextractor/wikiextractor/extract.py\", line 964, in clean_text\n",
            "    html_safe=html_safe)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/wikiextractor/wikiextractor/extract.py\", line 86, in clean\n",
            "    text = extractor.expandTemplates(text)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/wikiextractor/wikiextractor/extract.py\", line 86, in clean\n",
            "    text = extractor.expandTemplates(text)\n",
            "  File \"/usr/lib/python3.7/bdb.py\", line 88, in trace_dispatch\n",
            "    return self.dispatch_line(frame)\n",
            "  File \"/usr/lib/python3.7/bdb.py\", line 113, in dispatch_line\n",
            "    if self.quitting: raise BdbQuit\n",
            "bdb.BdbQuit\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"wikiextractor/WikiExtractor.py\", line 473, in extract_process\n",
            "    Extractor(*job[:-1]).extract(out, html_safe)  # (id, urlbase, title, page)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/wikiextractor/wikiextractor/extract.py\", line 976, in extract\n",
            "    text = self.clean_text(text, html_safe=html_safe)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/wikiextractor/wikiextractor/extract.py\", line 964, in clean_text\n",
            "    html_safe=html_safe)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/wikiextractor/wikiextractor/extract.py\", line 86, in clean\n",
            "    text = extractor.expandTemplates(text)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/wikiextractor/wikiextractor/extract.py\", line 86, in clean\n",
            "    text = extractor.expandTemplates(text)\n",
            "  File \"/usr/lib/python3.7/bdb.py\", line 88, in trace_dispatch\n",
            "    return self.dispatch_line(frame)\n",
            "  File \"/usr/lib/python3.7/bdb.py\", line 113, in dispatch_line\n",
            "    if self.quitting: raise BdbQuit\n",
            "bdb.BdbQuit\n"
          ]
        }
      ],
      "source": [
        "!python wikiextractor/WikiExtractor.py viwiki-20220401-pages-articles-multistream.xml.bz2 --processes 4 -o ./output/ --json\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmEJ7iWw549b",
        "outputId": "70fb563c-91b2-47b8-f231-30ec4d30409b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 16.9 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 81 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 92 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 5.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6CSCHaQiNX9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import unidecode\n",
        "import itertools\n",
        "from nltk import ngrams\n",
        "import string\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PATH_DATA = \"./output\"\n",
        "\n",
        "# alphabet = '^[ _abcdefghijklmnopqrstuvwxyz0123456789áàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵđ!\\\"\\',\\-\\.:;?_\\(\\)]+$'\n",
        "\n",
        "# list_sub_folder = os.listdir(PATH_DATA)\n",
        "# for sub_folder in (list_sub_folder):\n",
        "#     path_sub_folder = os.path.join(PATH_DATA, sub_folder)\n",
        "    \n",
        "#     list_file = os.listdir(path_sub_folder)\n",
        "    \n",
        "#     for file in tqdm(list_file):\n",
        "#         with open(os.path.join(path_sub_folder, file), \"r\") as f_r:\n",
        "#             contents = f_r.read()\n",
        "#             contents = re.sub(\"(\\s)+\", r\"\\1\", contents)\n",
        "#             contents = contents.split(\"\\n\")\n",
        "#             for content in contents:\n",
        "#                 try:\n",
        "#                     content = eval(content)\n",
        "#                 except:\n",
        "#                     continue\n",
        "#                 lines = content[\"text\"].split(\"\\n\")\n",
        "#                 with open(\"./train_data.txt\", \"a\") as f_w:\n",
        "#                     for line in lines[1:]:\n",
        "#                         if len(line.split()) > 2 and re.match(alphabet, line.lower()):\n",
        "#                             f_w.write(line + \"\\n\")"
      ],
      "metadata": {
        "id": "SejTY1OiRPI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwM_J2tPRWWX"
      },
      "outputs": [],
      "source": [
        "# with open(\"./train_data.txt\", \"r\") as f_r:\n",
        "#     lines = f_r.read().split(\"\\n\")\n",
        "    \n",
        "# print(len(lines))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUPYZOhSkCIt"
      },
      "outputs": [],
      "source": [
        "MAXLEN = 30\n",
        "NGRAM = 5\n",
        "BATCH_SIZE = 1024\n",
        "def remove_accent(text):\n",
        "    return unidecode.unidecode(text)\n",
        "def extract_phrases(text):\n",
        "    return re.findall(r'\\w[\\w ]+', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JGwOJcBRbcV"
      },
      "outputs": [],
      "source": [
        "# phrases = itertools.chain.from_iterable(extract_phrases(text) for text in lines)\n",
        "# phrases = [p.strip() for p in phrases if len(p.split()) > 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebI5PT67RgSd"
      },
      "outputs": [],
      "source": [
        "def gen_ngrams(words, n=5):\n",
        "    return ngrams(words.split(), n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8O6juXQRgrd"
      },
      "outputs": [],
      "source": [
        "# list_ngrams = []\n",
        "\n",
        "# for p in tqdm(phrases):\n",
        "#   for ngr in gen_ngrams(p, NGRAM):\n",
        "#     if len(\" \".join(ngr)) < 30:\n",
        "#       list_ngrams.append(\" \".join(ngr))\n",
        "# del phrases\n",
        "# list_ngrams = list(set(list_ngrams))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open('list_ngrams.data', 'rb') as filehandle:\n",
        "    # read the data as binary data stream\n",
        "    list_ngrams = pickle.load(filehandle)"
      ],
      "metadata": {
        "id": "-R6sO9yPRSvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdC4GrJ-kJ4Q"
      },
      "outputs": [],
      "source": [
        "accented_chars_vietnamese = [\n",
        "    'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ',\n",
        "    'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ',\n",
        "    'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ',\n",
        "    'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự',\n",
        "    'í', 'ì', 'ỉ', 'ĩ', 'ị',\n",
        "    'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ',\n",
        "    'đ', \n",
        "]\n",
        "accented_chars_vietnamese.extend([c.upper() for c in accented_chars_vietnamese])\n",
        "alphabet = list(('\\x00 _' + string.ascii_letters + string.digits + ''.join(accented_chars_vietnamese)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ4E6fVDkMrB"
      },
      "outputs": [],
      "source": [
        "def encode(text, maxlen=MAXLEN):\n",
        "        text = \"\\x00\" + text\n",
        "        x = np.zeros((maxlen, len(alphabet)))\n",
        "        for i, c in enumerate(text[:maxlen]):\n",
        "            x[i, alphabet.index(c)] = 1\n",
        "        if i < maxlen - 1:\n",
        "          for j in range(i+1, maxlen):\n",
        "            x[j, 0] = 1\n",
        "        return x\n",
        "\n",
        "def decode(x, calc_argmax=True):\n",
        "    if calc_argmax:\n",
        "        x = x.argmax(axis=-1)\n",
        "    return ''.join(alphabet[i] for i in x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-lry7WTkQpD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d546e85c-b144-4216-df8f-a333c3b74fb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 30, 256)           466944    \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 30, 512)          1050624   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 30, 199)          102087    \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 30, 199)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,619,655\n",
            "Trainable params: 1,619,655\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, TimeDistributed, Dense, RepeatVector, recurrent, LSTM, Bidirectional\n",
        "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "HIDDEN_SIZE = 256\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(HIDDEN_SIZE, input_shape=(MAXLEN, len(alphabet)), return_sequences=True))\n",
        "model.add(Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True, dropout=0.25, recurrent_dropout=0.1)))\n",
        "model.add(TimeDistributed(Dense(len(alphabet))))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pe15tPN0kS1T"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, valid_data = train_test_split(list_ngrams, test_size=0.4, random_state=2019)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJf0x4Ol_YAm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb9b522f-1f08-494c-8b20-14d3df827dca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25488871"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "len(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeSiA0u8oO6i"
      },
      "outputs": [],
      "source": [
        "def generate_data(data, batch_size=128):\n",
        "    cur_index = 0\n",
        "    while True:\n",
        "        \n",
        "        x, y = [], []\n",
        "        for i in range(batch_size):  \n",
        "            y.append(encode(data[cur_index]))\n",
        "            x.append(encode(unidecode.unidecode(data[cur_index])))\n",
        "            cur_index += 1\n",
        "            \n",
        "            if cur_index > len(data)-1:\n",
        "                cur_index = 0\n",
        "        \n",
        "        yield np.array(x), np.array(y)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9roJVufoQ17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a56db33-bebe-4b88-b752-3cbeef438516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   19/24891 [..............................] - ETA: 6:12:59 - loss: 3.6698 - accuracy: 0.3091"
          ]
        }
      ],
      "source": [
        "import os\n",
        "train_generator = generate_data(train_data, batch_size=BATCH_SIZE)\n",
        "validation_generator = generate_data(valid_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath=os.path.join('./model_{val_loss:.4f}_{val_acc:.4f}.h5'), save_best_only=True, verbose=1)\n",
        "early = EarlyStopping(patience=2, verbose=1)\n",
        "\n",
        "model.fit(train_generator, steps_per_epoch=len(train_data)//BATCH_SIZE, epochs=1,\n",
        "                    validation_data=validation_generator, validation_steps=len(valid_data)//BATCH_SIZE,\n",
        "                    callbacks=[checkpointer, early])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEepeW90usop"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "fill_VN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMRD6fi+AwlC7NkQ9c2tcdh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}